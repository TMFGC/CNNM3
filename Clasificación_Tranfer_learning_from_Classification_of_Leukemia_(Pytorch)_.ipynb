{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TMFGC/CNNM3/blob/main/Clasificaci%C3%B3n_Tranfer_learning_from_Classification_of_Leukemia_(Pytorch)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt9a018K8xOx"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylRAejVBxDtd"
      },
      "outputs": [],
      "source": [
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNJr7SqK86Mv"
      },
      "source": [
        "Importar librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3stKx-GNCDOt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHlt5bn29YKg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtJ92j1Ewteb"
      },
      "outputs": [],
      "source": [
        "# abrir archivo\n",
        "data_cells= '/content/drive/My Drive/datasets/dataset_m3/train_dir'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pye6PUcOA0Du"
      },
      "outputs": [],
      "source": [
        " os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFM_PMj1-nCy"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFHHl_If-orK"
      },
      "outputs": [],
      "source": [
        "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwleZFGa-to7"
      },
      "outputs": [],
      "source": [
        "torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W249UYA-1Jm"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpnxvFmm-3WH"
      },
      "source": [
        "Cargar el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCQVus9H-5_z"
      },
      "outputs": [],
      "source": [
        "# Paths and configurations\n",
        "data_cells = '/content/drive/My Drive/datasets/dataset_m3/train_dir'\n",
        "BATCH_SIZE = 16\n",
        "CLASSES = ['PMCA','SNE','MO','LY','BLAST']\n",
        "IMAGE_SIZE = (144, 144)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3xTO-Wz_DLA"
      },
      "outputs": [],
      "source": [
        "# Helper function to count files in directories\n",
        "def count_files_in_directory(root_directory, target_folder):\n",
        "    target_path = None\n",
        "    for root, dirs, _ in os.walk(root_directory):\n",
        "        if target_folder in dirs:\n",
        "            target_path = os.path.join(root, target_folder)\n",
        "            break\n",
        "    if target_path:\n",
        "        total_files = sum(len(files) for _, _, files in os.walk(target_path))\n",
        "        print(f\"Total number of files under the '{target_folder}' folder: {total_files}\")\n",
        "    else:\n",
        "        print(f\"No '{target_folder}' folder found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEtuxiiV_J2A"
      },
      "outputs": [],
      "source": [
        "for cls in CLASSES:\n",
        "    count_files_in_directory(data_cells, cls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTDKtOJ6xgaJ"
      },
      "source": [
        "CAMBIAR IMAGEN DE JPG A TIFF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elnfnj5Txl10"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def convert_jpg_to_tiff(data_cells, classes):\n",
        "  for cls in classes:\n",
        "    class_path = os.path.join(data_cells, cls)\n",
        "    for filename in os.listdir(class_path):\n",
        "      if filename.endswith(\".jpg\"):\n",
        "        filepath = os.path.join(class_path, filename)\n",
        "        with Image.open(filepath) as im:\n",
        "          # Crea un nuevo nombre de archivo con extensión .tiff\n",
        "          tiff_filepath = os.path.join(class_path, filename[:-4] + \".tiff\")\n",
        "          im.save(tiff_filepath, \"TIFF\")  # Guarda la imagen como TIFF\n",
        "        # (Opcional) Elimina el archivo JPG original si ya no lo necesitas\n",
        "        # os.remove(filepath)\n",
        "\n",
        "# Llama a la función para convertir las imágenes\n",
        "convert_jpg_to_tiff(data_cells, CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-B4l76KAVtC"
      },
      "outputs": [],
      "source": [
        "# Dataset class for loading images\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_cells, classes, image_batch_size=3000, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images, self.labels = self.load_images(data_cells, classes, image_batch_size)\n",
        "\n",
        "    def load_images(self, data_cells, classes, image_batch_size):\n",
        "        images, labels = [], []\n",
        "        for idx, class_name in enumerate(classes):\n",
        "            class_path = os.path.join(data_cells, class_name)\n",
        "            # Use os.listdir to get all files and filter by extension\n",
        "            # The problem was here, the '.tif' files were actually '.tiff' files\n",
        "            image_paths = [os.path.join(class_path, f) for f in os.listdir(class_path) if f.endswith('.tiff')]\n",
        "\n",
        "\n",
        "            # Removed the image_batch_size limitation to load all images\n",
        "            for img_path in image_paths:\n",
        "                with Image.open(img_path) as img:\n",
        "                    images.append(np.array(img.convert('RGB')))\n",
        "                    labels.append(idx)\n",
        "        return images, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.transform(self.images[idx]) if self.transform else self.images[idx]\n",
        "        return image, self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOsZkOfo_SiV"
      },
      "outputs": [],
      "source": [
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX230XVX_dKj"
      },
      "source": [
        "Crear y dividir un conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YY7iH4DoAAl8"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(data_cells=data_cells, classes=CLASSES, transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))  # Calcular val_size basado en el dataset completo\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size]) # Dividir en 3 conjuntos\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoRLMXcdC2yt"
      },
      "source": [
        "modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjoWQyb1CxTJ"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_layers, num_units, dropout_rate):\n",
        "        super(CNNModel, self).__init__()\n",
        "        layers = [nn.Conv2d(3, num_units, kernel_size=3, padding=1), nn.ReLU(),\n",
        "                  nn.BatchNorm2d(num_units), nn.MaxPool2d(2)]\n",
        "\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers.extend([nn.Conv2d(num_units, num_units * 2, kernel_size=3, padding=1),\n",
        "                           nn.ReLU(), nn.BatchNorm2d(num_units * 2), nn.MaxPool2d(2)])\n",
        "            num_units *= 2\n",
        "\n",
        "        self.conv_layers = nn.Sequential(*layers)\n",
        "        fc_input_size = num_units * (IMAGE_SIZE[0] // (2 ** num_layers)) * (IMAGE_SIZE[1] // (2 ** num_layers))\n",
        "        self.fc1 = nn.Linear(fc_input_size, 512)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(512, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x).view(x.size(0), -1)\n",
        "        return self.fc2(self.dropout(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ugf3ShC6zT"
      },
      "source": [
        "Hyperparameter Tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJrhy_J6C7me"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TO3fMDDC-7F"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 2, 3)\n",
        "    num_units = trial.suggest_int(\"num_units\", 128, 384, step=64)\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.3)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True)\n",
        "\n",
        "    model = CNNModel(num_layers=num_layers, num_units=num_units, dropout_rate=dropout_rate).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    patience = 3\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        train_loss = sum(criterion(model(inputs.to(device)), labels.to(device)).item() for inputs, labels in train_loader) / len(train_loader)\n",
        "        val_loss = sum(criterion(model(inputs.to(device)), labels.to(device)).item() for inputs, labels in val_loader) / len(val_loader)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                break\n",
        "    return best_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY84ZJ95DCwy",
        "outputId": "cb025af5-3dd1-4e07-bd7a-10c2c52be503"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-04 21:41:23,430] A new study created in memory with name: no-name-395731fd-608c-4a3e-86da-67987cb66f73\n",
            "[I 2025-05-04 21:53:41,424] Trial 0 finished with value: 1.589538186788559 and parameters: {'num_layers': 3, 'num_units': 192, 'dropout_rate': 0.14571073604503323, 'learning_rate': 6.024685934644941e-05}. Best is trial 0 with value: 1.589538186788559.\n",
            "[I 2025-05-04 22:13:09,853] Trial 1 finished with value: 1.5798932909965515 and parameters: {'num_layers': 2, 'num_units': 384, 'dropout_rate': 0.028518823772527034, 'learning_rate': 2.7913958206714023e-05}. Best is trial 1 with value: 1.5798932909965515.\n",
            "[I 2025-05-04 22:20:59,753] Trial 2 finished with value: 1.6125814020633698 and parameters: {'num_layers': 3, 'num_units': 128, 'dropout_rate': 0.22556623846390528, 'learning_rate': 3.928759344255263e-05}. Best is trial 1 with value: 1.5798932909965515.\n",
            "[I 2025-05-04 22:29:07,267] Trial 3 finished with value: 1.4870198965072632 and parameters: {'num_layers': 3, 'num_units': 192, 'dropout_rate': 0.2805527552760463, 'learning_rate': 1.8334871179735218e-05}. Best is trial 3 with value: 1.4870198965072632.\n",
            "[I 2025-05-04 22:51:58,506] Trial 4 finished with value: 1.6094220280647278 and parameters: {'num_layers': 3, 'num_units': 320, 'dropout_rate': 0.20190898468070792, 'learning_rate': 1.08170815474178e-05}. Best is trial 3 with value: 1.4870198965072632.\n",
            "[I 2025-05-04 23:03:36,330] Trial 5 finished with value: 1.670683741569519 and parameters: {'num_layers': 2, 'num_units': 320, 'dropout_rate': 0.17464947605393868, 'learning_rate': 9.09147724816599e-05}. Best is trial 3 with value: 1.4870198965072632.\n"
          ]
        }
      ],
      "source": [
        "# Run Optuna optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IogktgOyDKTl"
      },
      "outputs": [],
      "source": [
        "best_params = study.best_params\n",
        "\n",
        "print(\"  Hyperparameters: \")\n",
        "for key, value in best_params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-hqHVkPDQ1B"
      },
      "source": [
        "Training with Best Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMve9tKeDOjG"
      },
      "outputs": [],
      "source": [
        "best_model = CNNModel(\n",
        "    num_layers=best_params['num_layers'],\n",
        "    num_units=best_params['num_units'],\n",
        "    dropout_rate=best_params['dropout_rate']\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVaZnsUiDUAZ"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKwaR7HNDWFO"
      },
      "outputs": [],
      "source": [
        "# Metrics from training\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "EPOCHS = 50\n",
        "patience = 3\n",
        "best_val_loss = float('inf')\n",
        "epochs_without_improvement = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj6XLMxQDYOn"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    # Training phase\n",
        "    best_model.train()\n",
        "    running_train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = best_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "    train_accuracy = correct_train / total_train\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation phase\n",
        "    best_model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = best_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    avg_val_loss = running_val_loss / len(val_loader)\n",
        "    val_accuracy = correct_val / total_val\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "          f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
        "\n",
        "    # Early stopping check\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl9xZDXRDcCU"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation losses and accuracies\n",
        "def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n",
        "    epochs_range = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Plot Losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    # Plot Accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tTJgq3fDhsp"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tomK7icBDiUH"
      },
      "outputs": [],
      "source": [
        "y_pred, y_true = [], []\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs.to(device))\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifWZBLRpDmTO"
      },
      "outputs": [],
      "source": [
        "evaluate_model(best_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCLHOdrEDoOd"
      },
      "outputs": [],
      "source": [
        "m = confusion_matrix(y_true, y_pred)\n",
        "cm_normalized = cm / cm.sum(axis=1)[:, np.newaxis]  # Normalize by row (true condition)\n",
        "\n",
        "# Plot the normalized confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=CLASSES)\n",
        "disp.plot(cmap=plt.cm.Blues, values_format=\".0%\")\n",
        "plt.title('Normalized Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAqxacihDp-l"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true, y_pred, target_names=CLASSES))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}